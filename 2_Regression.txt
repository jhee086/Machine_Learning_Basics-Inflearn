<3> 선형 회귀 "H(x) = Wx + b"

<개념>
    : 회귀식에서 MSE를 통해 가장 적합한 w와 b를 찾는 것 (회귀: 평균으로 돌아가다)
    : 지도학습(레이블[정답] 존재O)의 대표적인 예시
    : Hypothesis -> H(x)) = Wx + b
    : MSE(2차 함수)
        -> 회귀선과 데이터 간 잔차의 제곱을 이용하는 MSE 
        -> 잔차(residual): 표본으로 부터 추정한 회귀식과 실제 관측값 차이

<대표적인 방법: 경사하강법>
    : cost func 값이 최소가 되게 하는 최적의 w와 b값을 찾는 것
    -> 편미분, 접선의 기울기가 0이 될 때 최적의 w 값
    * 주의사항
        1) learning rate: 기울기의 정도
            -> 너무 작으면 시간이 오래 걸림
            -> 너무 크면 최적의 값 지나칠 가능성
        2) 기울기가 0이 되는 개수가 많을 경우
            -> global minimum vs local minimum 
                => convex optimization (볼록 최적화)


<4> 다중 선형 회귀 "H(x1, x2) = W1x1 + W2x2 + b"

<주의>
    : 모든 변수를 다 투입하여 모델을 돌리지 않음(데이터들을 골라내는 것도 중요)
    * 데이터 추려내는데 사용되는 기법
        1) 비지도 학습-시각화 
        2) PCA
        3) 상관관계분석

<대표적으로 사용하는 것>
    1. 전진선택법
        : 상수모형부터 시작하여 중요하다고 여겨지는 설명벼수부터 차례대로 모델에 추가
    2. 후진제거법
        : 가장 덜 영향을 준다고 생각하는 변수를 전체에서 1개씩 제거함
    ex) x 변수가 30개 이상 -> 최종적으로 사용한 데이터 변수 10개


<5> 선형 회귀 모델 구현 / colab(Google Colaboratory) - Google Drive + Jupyter
    - ML[Python] -> Anaconda, Jupyter
    - TPU: Google에서 2016.05에 발표한 데이터 분석 및 딥러닝 하드웨어

    !python --version

    a = np.array([5, 10, 15, 20])
    b = np.array([15, 20, 34, 50])

    result = pd.DataFrame({
        "a" : a, 
        "b" : b
    })
    result

    plt.plot(a, b, '*')

    height = np.array([183, 165, 180, 192, 172, 156])
    height = height.reshape(-1, 1) # 2차원 값이 들어가기 위해 reshape

    math = np.array([85, 45, 80, 99, 75, 45])

    from sklearn.linear_model import LinearRegression

    line_fitter = LinearRegression()

    line_fitter.fit(height, math) 
    # fit()함수: line_fitter.coef_ -> 기울기 저장
    #            line_fitter.intercept_ -> 절편 저장

    score_predict = line_fitter.predict(height)

    plt.plot(height, math, '*')
    plt.plot(height, score_predict)
    plt.show

    line_fitter.coef_

    line_fitter.intercept_

    # 성능 평가 -> MSE
    from sklearn.metrics import mean_squared_error

    print("Mean_Squared_Error:" , mean_squared_error(score_predict, math))
    print("RMSE:" , (mean_squared_error(score_predict, math)**0.5))
    print("score:", line_fitter.score(height, math))


<6> 로지스틱 회귀

<개념>
    : 1 또는 0인 이진형 변수에서 쓰이는 회귀분석 방법 (분류하는데 사용)
    : p = ax + b (좌변: 0 or 1, 우변: 마이너스 무한대 ~ 플러스 무한대)
        1) p대신 Odds 넣기 
            * Odds란: 성공확률/실패확률 = 성공확률/(1-성공확률) = p/1-p 
        2) Odds에 로그 씌우기
            -> ln(Odds) = ln(p/1-p) = ax + b
        3) 양변에 e 제곱
            -> p = e^(ax+b) / 1 + e^(ax+b)


<7> 로지스틱 회귀 모델 구현

{{Colab 참고}}