<8> 랜덤포레스트

<결정트리>
    1. 분류와 회귀 모두 가능한 지도 학습 모델
    2. Y or N 질문들로 이어가며 학습
    3. 특정 질문에 따라 데이터를 구분하는 학습
    4. 한번의 분기 때마다 두개로 구분
    5. Node: 질문이나 정답의 네모 상자
    6. Root Node: 제일 상단 분류 기준
    7. Terminal/Leaf Node: 제일 하단 분류 기준

<결정트리 프로세스> 
    1. 질문들을 기준으로 나눔
    2. 나눈 범주에서 또 다시 질문들을 기준으로 나눔
        -> 지나치게 나누면 오버피팅 가능성
        => 이를 막기 위해 "가지치기"
            * 가지치기 종류 
                1) 사전 가지치기(pre-pruning)
                    : 트리 생성을 일찍 중단 시킴
                    : 트리에 최대 깊이 or Leaf Node의 최대 개수 제한
                    : 노드가 분할하기 위한 포인트에 최소 1개 지정
                2) 사후 가지치기(post-pruning)
                    : 트리 완성 후 데이터 포인트가 적은 노드를 삭제 or 병합
    3. 특성 중요도 (전체합은 1)
        : 트리를 만드는 결정에 각 특성이 얼마나 중요한지를 봄
        : 0과 1사이의 숫자 
            0 - 각 특성에 대해 전혀 사용되지 않음
            1 - 완벽하게 타겟 예측

<장점>
    1. 알고리즘이 직관적
    2. 룰이 뚜렷하며 규칙 노드와 리프 노드를 알기 쉽고 규칙 파악도 용이
    3. 전처리 작업이 덜 필요
    4. 이상치, 노이즈에 큰 영향 받지 않음
    5. 선형성, 정규성, 등분산성 가정 필요 없음
    6. 교호효과: 2개 이상의 변수가 조합해 y에 어떻게 영향을 끼치는지 자동적으로 알려줌
        -> 중요변수(어떤 것이 제일 영향을 많이 미쳤는지)를 알기 편함
    7. 연속형, 이산형 데이터 둘 다 다룰 수 있음

<단점>
    1. 과적합 가능성 -> 가지치기로 해결
    2. 모델의 유연성 떨어짐
    3. 학습 데이터에 따라 차이가 있음 (일반성 부족) -> 정확도 떨어짐


<9> 의사 결정 나무 구현

{{Colab 참고}}


<10> 랜덤 포레스트: 여러 결정 트리의 묶음

<앙상블(Ensemble)>
    : 여러 머신러닝 모델을 연결하여 더 강력한 모델을 만드는 기법
    * 대표적인 두가지
        1. 배깅(bagging): 샘플을 여러번 뽑아 학습시켜 결과물을 집계
        2. 부스팅(Boosting): 가중치 활용하여 약분류기를 강분류기로 만듦

<랜덤 포레스트 프로세스>
    1. 결정 트리 많이 만들기/트리 생성에 무작위성 주입
    2. 각 트리가 고유하게 만들어지도록 무작위 선택 
        - Bootstrap Sample: 첫번째 단계에서 무작위 중복을 허용하여 선택한 n개의 데이터 
    3. 기존 트리와 달리 무작위로 선택 후 후보들 중 최선의 테스트 도출
    4. Max Feature: 후보 특성의 최대 추출 개수를 전체 추출 개수로 설정
        -> 모든 특성 고려하여 무작위성 고려 X
        -> 크게하면 각 트리들이 비슷해져 가장 두드러진 특성 기준으로 데이터 맞춰짐
        -> 작게하면 데이터를 맞추기 위해 깊이가 깊어짐


<11> 랜덤 포레스트 구현

{{Colab 참고}}